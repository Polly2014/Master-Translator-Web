#!/usr/bin/env python3
"""
Master Translator Web Application
æ™ºèƒ½åˆ†å—ç¿»è¯‘çš„ Web ç•Œé¢ - Hackathon Demo ç‰ˆ
"""

import os
import json
import time
import re
from pathlib import Path
from datetime import datetime
from flask import Flask, render_template, request, jsonify, send_file
from flask_socketio import SocketIO, emit
from flask_cors import CORS
from werkzeug.utils import secure_filename
import threading
from litellm import completion
import os
from dotenv import load_dotenv
from docx import Document
from markdownify import markdownify as md

# è‡ªåŠ¨åŠ è½½ .env æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
load_dotenv()

# ============ Flask é…ç½® ============
app = Flask(__name__)
app.config['SECRET_KEY'] = 'master-translator-secret-2024'
app.config['UPLOAD_FOLDER'] = Path('./uploads')
app.config['OUTPUT_FOLDER'] = Path('./outputs')
app.config['MAX_CONTENT_LENGTH'] = 50 * 1024 * 1024  # 50MB

CORS(app)
socketio = SocketIO(app, cors_allowed_origins="*", async_mode='threading')

# ç¡®ä¿ç›®å½•å­˜åœ¨
app.config['UPLOAD_FOLDER'].mkdir(exist_ok=True)
app.config['OUTPUT_FOLDER'].mkdir(exist_ok=True)

# ============ ç¿»è¯‘é…ç½® ============
# ä»ç¯å¢ƒå˜é‡è¯»å– OpenRouter API Keyï¼Œé¿å…ç¡¬ç¼–ç æ³„æ¼
OPENROUTER_API_KEY = os.environ.get("OPENROUTER_API_KEY")
if not OPENROUTER_API_KEY:
    print("[WARN] OPENROUTER_API_KEY æœªè®¾ç½®ï¼Œç¿»è¯‘æ¥å£å°†ä¸å¯ç”¨ã€‚è¯·åœ¨æœ¬åœ°åˆ›å»º .env æˆ–é€šè¿‡ shell å¯¼å‡ºå˜é‡ã€‚")

# ============ æ¨¡å‹é…ç½®å­—å…¸ ============
MODEL_CONFIGS = {
    'deepseek-free': {
        'name': 'tngtech/deepseek-r1t-chimera:free',
        'max_tokens': 16000,
        'temperature': 0.3,
        'cost_per_1k': 0.0,
        'description': 'å…è´¹æ¨¡å‹ï¼Œé€‚åˆ Demo å’Œå¼€å‘æµ‹è¯•',
        'speed': 'fast',
        'quality': 'good'
    },
    'claude-sonnet-4': {
        'name': 'anthropic/claude-sonnet-4',
        'max_tokens': 100000,
        'temperature': 0.3,
        'cost_per_1k': 0.01,
        'description': 'æœ€é«˜è´¨é‡ï¼Œé€‚åˆç”Ÿäº§ç¯å¢ƒ',
        'speed': 'medium',
        'quality': 'excellent'
    },
    'gpt-4o': {
        'name': 'openai/gpt-4o',
        'max_tokens': 100000,
        'temperature': 0.3,
        'cost_per_1k': 0.0067,
        'description': 'å¹³è¡¡æ€§èƒ½å’Œæˆæœ¬',
        'speed': 'fast',
        'quality': 'excellent'
    },
    'deepseek-v3': {
        'name': 'deepseek/deepseek-chat',
        'max_tokens': 64000,
        'temperature': 0.3,
        'cost_per_1k': 0.0013,
        'description': 'é«˜æ€§ä»·æ¯”ï¼Œé€‚åˆå¤§è§„æ¨¡ç”Ÿäº§',
        'speed': 'very-fast',
        'quality': 'good'
    }
}

# å½“å‰ä½¿ç”¨çš„æ¨¡å‹ï¼ˆä¿®æ”¹è¿™é‡Œåˆ‡æ¢æ¨¡å‹)
ACTIVE_MODEL = 'deepseek-free'  # å¯é€‰: deepseek-free, claude-sonnet-4, gpt-4o, deepseek-v3

# ä»é…ç½®ä¸­åŠ è½½å½“å‰æ¨¡å‹å‚æ•°
current_config = MODEL_CONFIGS[ACTIVE_MODEL]
MODEL = current_config['name']
MAX_TOKENS = current_config['max_tokens']
TEMPERATURE = current_config['temperature']
TIMEOUT = 3600

# ============ åˆ†å—é…ç½® ============
# ä½¿ç”¨ä¸“é—¨çš„ Demo æ–‡ä»¶ (demo_files/) è¿›è¡Œæ¼”ç¤º
# Ultra Quick Demo: 200 words (~20-30s, 3 chunks)
# Standard Demo: 1,037 words (~3-5min)
DEMO_MODE = True  # å·²æœ‰ä¸“é—¨ Demo æ–‡ä»¶ï¼Œä½¿ç”¨ç”Ÿäº§é…ç½®

if DEMO_MODE:
    CHUNK_TARGET_SIZE = 800          # Demo: è¶…å°å—ï¼Œç¡®ä¿ Quick Demo 3ç« â†’3å—ï¼ˆ~20-30sï¼‰
    CONTEXT_PARAGRAPHS = 1           # Demo: å‡å°‘ä¸Šä¸‹æ–‡ï¼ŒåŠ å¿«é€Ÿåº¦
    OVERLAP_CHECK_CHARS = 100        # Demo: å‡å°‘é‡å æ£€æŸ¥
else:
    CHUNK_TARGET_SIZE = 110000       # ç”Ÿäº§: å¤§å—ï¼Œå‡å°‘ API è°ƒç”¨
    CONTEXT_PARAGRAPHS = 2           # ç”Ÿäº§: æ›´å¤šä¸Šä¸‹æ–‡ï¼Œæé«˜è´¨é‡
    OVERLAP_CHECK_CHARS = 200        # ç”Ÿäº§: æ›´å¤šé‡å æ£€æŸ¥

# å…¶ä»–é…ç½®

LANGUAGES = {
    # East Asian
    'Chinese': 'ä¸­æ–‡ (ç®€ä½“)',
    'Traditional Chinese': 'ä¸­æ–‡ (ç¹ä½“)',
    'Japanese': 'æ—¥è¯­',
    'Korean': 'éŸ©è¯­',
    
    # European
    'French': 'æ³•è¯­',
    'German': 'å¾·è¯­',
    'Spanish': 'è¥¿ç­ç‰™è¯­',
    'Italian': 'æ„å¤§åˆ©è¯­',
    'Portuguese': 'è‘¡è„ç‰™è¯­',
    'Russian': 'ä¿„è¯­',
    'Polish': 'æ³¢å…°è¯­',
    'Dutch': 'è·å…°è¯­',
    'Swedish': 'ç‘å…¸è¯­',
    
    # Middle Eastern & South Asian
    'Arabic': 'é˜¿æ‹‰ä¼¯è¯­',
    'Hebrew': 'å¸Œä¼¯æ¥è¯­',
    'Hindi': 'å°åœ°è¯­',
    'Urdu': 'ä¹Œå°”éƒ½è¯­',
    'Persian': 'æ³¢æ–¯è¯­',
    'Turkish': 'åœŸè€³å…¶è¯­',
    
    # Southeast Asian
    'Thai': 'æ³°è¯­',
    'Vietnamese': 'è¶Šå—è¯­',
    'Indonesian': 'å°å°¼è¯­',
    'Malay': 'é©¬æ¥è¯­',
    'Spanish': 'è¥¿ç­ç‰™è¯­',
    'German': 'å¾·è¯­'
}

# ============ å…¨å±€ä»»åŠ¡ç®¡ç† ============
tasks = {}  # å­˜å‚¨æ‰€æœ‰ç¿»è¯‘ä»»åŠ¡


class TranslationTask:
    """ç¿»è¯‘ä»»åŠ¡ç±»"""
    def __init__(self, task_id, filename, language):
        self.task_id = task_id
        self.filename = filename
        self.language = language
        self.status = 'pending'  # pending, analyzing, translating, completed, failed
        self.progress = 0
        self.current_chunk = 0
        self.total_chunks = 0
        self.chunks_info = []
        self.logs = []
        self.source_content = ""
        self.result_file = None
        self.start_time = None
        self.end_time = None
        self.error = None
        self.use_terminology = True  # é»˜è®¤ä½¿ç”¨æœ¯è¯­æ•°æ®åº“
        
    def emit_log(self, message, level='info', update_last=False):
        """å‘é€æ—¥å¿—åˆ°å‰ç«¯
        
        Args:
            message: æ—¥å¿—æ¶ˆæ¯
            level: æ—¥å¿—çº§åˆ« (info, success, error, warning, progress)
            update_last: æ˜¯å¦æ›´æ–°æœ€åä¸€æ¡æ—¥å¿—ï¼ˆç”¨äºè¿›åº¦æ›´æ–°)
        """
        log_entry = {
            'timestamp': datetime.now().strftime('%H:%M:%S'),
            'message': message,
            'level': level,
            'update_last': update_last
        }
        self.logs.append(log_entry)
        socketio.emit('log', log_entry, room=self.task_id)
        
    def emit_progress(self, progress, chunk_progress=0):
        """å‘é€è¿›åº¦åˆ°å‰ç«¯"""
        self.progress = progress
        socketio.emit('progress', {
            'overall': progress,
            'chunk': chunk_progress,
            'current_chunk': self.current_chunk,
            'total_chunks': self.total_chunks
        }, room=self.task_id)


# ============ ç¿»è¯‘æ ¸å¿ƒå‡½æ•°ï¼ˆæ”¹é€ è‡ª script_v3_chunked.py)============

def convert_docx_to_markdown(docx_path):
    """
    å°† Word æ–‡æ¡£è½¬æ¢ä¸º Markdown æ ¼å¼
    
    Args:
        docx_path: Word æ–‡æ¡£è·¯å¾„
        
    Returns:
        str: Markdown æ ¼å¼çš„æ–‡æœ¬å†…å®¹
    """
    try:
        doc = Document(docx_path)
        markdown_content = []
        
        for para in doc.paragraphs:
            # å¤„ç†æ ‡é¢˜
            if para.style.name.startswith('Heading'):
                level = int(para.style.name.split()[-1]) if para.style.name.split()[-1].isdigit() else 1
                markdown_content.append(f"{'#' * level} {para.text}\n")
            # å¤„ç†æ™®é€šæ®µè½
            elif para.text.strip():
                markdown_content.append(f"{para.text}\n")
        
        # å¤„ç†è¡¨æ ¼
        for table in doc.tables:
            markdown_content.append("\n")
            for i, row in enumerate(table.rows):
                cells = [cell.text.strip() for cell in row.cells]
                markdown_content.append("| " + " | ".join(cells) + " |")
                # æ·»åŠ è¡¨å¤´åˆ†éš”ç¬¦
                if i == 0:
                    markdown_content.append("| " + " | ".join(["---"] * len(cells)) + " |")
            markdown_content.append("\n")
        
        result = "\n".join(markdown_content)
        
        # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
        result = re.sub(r'\n{3,}', '\n\n', result)
        
        return result.strip()
        
    except Exception as e:
        raise Exception(f"DOCX conversion failed: {str(e)}")


def extract_chapters(content):
    """æå–ç« èŠ‚ç»“æ„"""
    pattern = r'^(#{1,2}) (.+)$'
    chapters = []
    lines = content.split('\n')
    
    for i, line in enumerate(lines):
        match = re.match(pattern, line)
        if match:
            level = len(match.group(1))
            title = match.group(2).strip()
            start_pos = sum(len(l) + 1 for l in lines[:i])
            chapters.append({
                'line': i + 1,
                'level': level,
                'title': title,
                'start_pos': start_pos
            })
    
    total_len = len(content)
    for i, ch in enumerate(chapters):
        if i < len(chapters) - 1:
            ch['chars'] = chapters[i+1]['start_pos'] - ch['start_pos']
            ch['end_pos'] = chapters[i+1]['start_pos']
        else:
            ch['chars'] = total_len - ch['start_pos']
            ch['end_pos'] = total_len
    
    return chapters


def plan_chunks(chapters, content):
    """è§„åˆ’åˆ†Chunk - æ”¯æŒä»»ä½• Markdown æ–‡ä»¶ç»“æ„"""
    chunks = []
    current_chunk_chapters = []
    current_size = 0
    
    # ä¼˜å…ˆä½¿ç”¨ level 2 æ ‡é¢˜ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ level 1
    main_chapters = [c for c in chapters if c['level'] == 2]
    if not main_chapters:
        main_chapters = [c for c in chapters if c['level'] == 1]
    
    # å¦‚æœä»ç„¶æ²¡æœ‰ç« èŠ‚ï¼ŒæŒ‰å›ºå®šå¤§å°åˆ†å—æ•´ä¸ªæ–‡æ¡£
    if not main_chapters:
        total_len = len(content)
        num_chunks = max(1, (total_len + CHUNK_TARGET_SIZE - 1) // CHUNK_TARGET_SIZE)
        for i in range(num_chunks):
            start = i * CHUNK_TARGET_SIZE
            end = min(start + CHUNK_TARGET_SIZE, total_len)
            chunks.append({
                'id': i + 1,
                'chapters': [f'Segment {i+1}'],
                'start_pos': start,
                'end_pos': end,
                'size': end - start,
                'content': content[start:end]
            })
        return chunks
    
    if main_chapters:
        first_chapter_pos = main_chapters[0]['start_pos']
        prologue_content = content[:first_chapter_pos].strip()
        prologue_size = len(prologue_content)
    else:
        prologue_content = ""
        prologue_size = 0
    
    if main_chapters:
        last_chapter_pos = main_chapters[-1]['end_pos']
        epilogue_content = content[last_chapter_pos:].strip()
        epilogue_size = len(epilogue_content)
    else:
        epilogue_content = ""
        epilogue_size = 0
    
    for chapter in main_chapters:
        if current_size > 0 and current_size + chapter['chars'] > CHUNK_TARGET_SIZE:
            chunk_start = current_chunk_chapters[0]['start_pos']
            chunk_end = current_chunk_chapters[-1]['end_pos']
            chunks.append({
                'id': len(chunks) + 1,
                'chapters': [c['title'] for c in current_chunk_chapters],
                'start_pos': chunk_start,
                'end_pos': chunk_end,
                'size': current_size,
                'content': content[chunk_start:chunk_end]
            })
            current_chunk_chapters = []
            current_size = 0
        
        current_chunk_chapters.append(chapter)
        current_size += chapter['chars']
    
    if current_chunk_chapters:
        chunk_start = current_chunk_chapters[0]['start_pos']
        chunk_end = current_chunk_chapters[-1]['end_pos']
        chunks.append({
            'id': len(chunks) + 1,
            'chapters': [c['title'] for c in current_chunk_chapters],
            'start_pos': chunk_start,
            'end_pos': chunk_end,
            'size': current_size,
            'content': content[chunk_start:chunk_end]
        })
    
    if chunks and prologue_content:
        chunks[0]['content'] = prologue_content + "\n\n" + chunks[0]['content']
        chunks[0]['size'] += prologue_size + 2
        chunks[0]['has_prologue'] = True
        chunks[0]['prologue_size'] = prologue_size
    
    if chunks and epilogue_content:
        chunks[-1]['content'] = chunks[-1]['content'] + "\n\n" + epilogue_content
        chunks[-1]['size'] += epilogue_size + 2
        chunks[-1]['has_epilogue'] = True
        chunks[-1]['epilogue_size'] = epilogue_size
    
    return chunks


# ============ æ¨¡å‹ç®¡ç†å‡½æ•° ============

def get_model_info():
    """è·å–å½“å‰æ¨¡å‹é…ç½®ä¿¡æ¯"""
    config = MODEL_CONFIGS[ACTIVE_MODEL]
    return {
        'active_model': ACTIVE_MODEL,
        'model_name': config['name'],
        'max_tokens': config['max_tokens'],
        'temperature': config['temperature'],
        'cost_per_1k': config['cost_per_1k'],
        'description': config['description'],
        'speed': config['speed'],
        'quality': config['quality']
    }


def list_available_models():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹"""
    return {
        key: {
            'name': config['name'],
            'description': config['description'],
            'cost': f"${config['cost_per_1k']:.4f}/1K chars",
            'speed': config['speed'],
            'quality': config['quality']
        }
        for key, config in MODEL_CONFIGS.items()
    }


def load_terminology_db():
    """åŠ è½½ç²¾é€‰æœ¯è¯­æ•°æ®åº“"""
    term_file = Path(__file__).parent / 'terminology_curated.json'
    
    if term_file.exists():
        with open(term_file, 'r', encoding='utf-8') as f:
            terminology = json.load(f)
        
        all_terms = []
        for category, terms in terminology.items():
            all_terms.extend(terms)
        
        return all_terms
    return None


def extract_terminology_from_chunk(translation_text, source_text):
    """ä»ç¿»è¯‘å—ä¸­æå–æ–°æœ¯è¯­ï¼ˆåŠ¨æ€æå–)"""
    import re
    
    # 1. æå–ä¸“æœ‰åè¯ï¼ˆå¤§å†™å¼€å¤´çš„è¯ç»„)
    proper_nouns = re.findall(r'\b[A-Z][A-Za-z]+(?:\s+[A-Z][A-Za-z]+){0,3}\b', source_text)
    
    # è¿‡æ»¤å¸¸è§è¯
    common_words = {
        'The', 'This', 'That', 'These', 'Those', 'Chapter', 'Part', 'Section',
        'And', 'But', 'For', 'With', 'When', 'Where', 'Which', 'What', 'How',
        'Figure', 'Table', 'Some', 'Many', 'Most', 'All', 'Each', 'Every',
        'First', 'Second', 'Third', 'Last', 'Next', 'Previous', 'Introduction',
        'Conclusion', 'Summary', 'Overview', 'Today', 'Tomorrow', 'Yesterday'
    }
    proper_nouns = [t for t in set(proper_nouns) if t not in common_words and len(t) > 2]
    
    # 2. æ£€æµ‹æŠ€æœ¯æœ¯è¯­ï¼ˆå¸¸è§æŠ€æœ¯è¯æ±‡åœ¨æºæ–‡æœ¬ä¸­å‡ºç°)
    tech_keywords = [
        'AI', 'ML', 'API', 'GPU', 'CPU', 'DNA', 'RNA', 'AGI',
        'artificial intelligence', 'machine learning', 'deep learning',
        'neural network', 'algorithm', 'model', 'dataset', 'training',
        'inference', 'transformer', 'attention', 'backpropagation',
        'reinforcement learning', 'supervised learning', 'unsupervised learning',
        'natural language processing', 'computer vision', 'robotics',
        'blockchain', 'cryptocurrency', 'quantum computing',
        'biotechnology', 'synthetic biology', 'gene editing', 'CRISPR'
    ]
    
    found_tech = [term for term in tech_keywords if term.lower() in source_text.lower()]
    
    # åˆå¹¶å¹¶å»é‡
    extracted_terms = list(set(proper_nouns + found_tech))
    
    return extracted_terms


def get_context_from_previous(prev_translation):
    """ä»å‰ä¸€å—ç¿»è¯‘æœ«å°¾æå–ä¸Šä¸‹æ–‡"""
    if not prev_translation:
        return ""
    
    paragraphs = [p.strip() for p in prev_translation.split('\n\n') if p.strip()]
    context_paras = paragraphs[-CONTEXT_PARAGRAPHS:] if len(paragraphs) >= CONTEXT_PARAGRAPHS else paragraphs
    context = '\n\n'.join(context_paras)
    
    return context


def translate_chunk_web(task, chunk_id, total_chunks, chunk_content, language, 
                        prev_context="", terminology=None):
    """Webç‰ˆç¿»è¯‘å•å—ï¼ˆå¸¦å®æ—¶æ—¥å¿—)"""
    
    task.emit_log(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”", 'info')
    task.emit_log(f"ğŸ”„ Starting chunk {chunk_id}/{total_chunks}", 'info')
    task.emit_log(f"ğŸ“ Input size: {len(chunk_content):,} characters", 'info')
    
    # Demo ä¼˜åŒ–ï¼šå¿«é€Ÿæ˜¾ç¤ºå¼€å§‹ä¿¡æ¯
    if DEMO_MODE:
        task.emit_log(f"âš¡ Demo mode: Using small chunks for quick demonstration", 'info')
    
    # æ˜¾ç¤ºä½¿ç”¨çš„æœ¯è¯­æ•°é‡
    if terminology:
        task.emit_log(f"ï¿½ Using {len(terminology)} terms for consistency", 'info')
    
    system_prompt = f"""You are a professional book translator. Translate the following book excerpt from English to {language}.

CRITICAL REQUIREMENTS:
1. **Preserve ALL markdown formatting**: headers (#, ##, ###), lists, quotes (>), code blocks, links
2. **Maintain structure**: Keep all chapters, sections, paragraphs exactly as structured
3. **Preserve names**: Keep all person names, company names, book titles in original English
4. **Technical terms**: Translate technical terms accurately, add English in parentheses for first occurrence if needed
5. **Natural language**: Use native {language} expression, not word-by-word translation
6. **Completeness**: Translate EVERY sentence, don't skip any content
7. **Consistency**: Maintain consistent terminology throughout the book

CONTEXT: This is chunk {chunk_id}/{total_chunks} of the complete book.
"""
    
    context_info = ""
    if prev_context:
        context_info = f"""
<previous_context>
For continuity, here are the last paragraphs from the previous chunk:
{prev_context}
</previous_context>
"""
    
    term_info = ""
    if terminology:
        term_info = f"""
<key_terminology>
Important terms to maintain consistency:
{', '.join(terminology[:25])}
</key_terminology>
"""
    
    user_prompt = f"""{context_info}{term_info}

Now translate this section to {language}:

---BEGIN CONTENT---

{chunk_content}

---END CONTENT---"""

    try:
        start_time = time.time()
        translated_text = ""
        last_update = start_time
        
        response = completion(
            model=f"openrouter/{MODEL}",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            api_key=OPENROUTER_API_KEY,
            max_tokens=MAX_TOKENS,
            temperature=TEMPERATURE,
            timeout=TIMEOUT,
            stream=True,
            extra_headers={
                "HTTP-Referer": "https://github.com/Polly2014",
                "X-Title": "Master Translator Web"
            }
        )
        
        for chunk in response:
            if hasattr(chunk, 'choices') and len(chunk.choices) > 0:
                delta = chunk.choices[0].delta
                if hasattr(delta, 'content') and delta.content:
                    translated_text += delta.content
                    
                    # Demo æ¨¡å¼ï¼šæ›´é¢‘ç¹æ›´æ–°ï¼ˆæ¯1seconds)ï¼Œç”Ÿäº§æ¨¡å¼ï¼šæ¯5seconds
                    update_interval = 1 if DEMO_MODE else 5
                    chars_threshold = 2000 if DEMO_MODE else 10000
                    
                    now = time.time()
                    if now - last_update > update_interval or len(translated_text) % chars_threshold < 100:
                        elapsed = now - start_time
                        speed = len(translated_text) / elapsed if elapsed > 0 else 0
                        chunk_progress = min(95, int((len(translated_text) / (len(chunk_content) * 1.5)) * 100))
                        
                        task.emit_progress(
                            progress=int(((chunk_id - 1) + chunk_progress / 100) / total_chunks * 100),
                            chunk_progress=chunk_progress
                        )
                        # ä½¿ç”¨ update_last=True æ›´æ–°è¿›åº¦æ¶ˆæ¯è€Œä¸æ˜¯è¿½åŠ 
                        task.emit_log(f"ğŸ“¥ Receiving translation... {len(translated_text):,} characters ({speed:.0f} c/s)", 'progress', update_last=True)
                        last_update = now
        
        elapsed = time.time() - start_time
        speed = len(translated_text) / elapsed if elapsed > 0 else 0
        
        task.emit_log(f"âœ… Chunk {chunk_id} completed: {len(translated_text):,} characters ({speed:.0f} c/s, {elapsed:.0f}s)", 'success')
        
        return translated_text
        
    except Exception as e:
        task.emit_log(f"âŒ Chunk {chunk_id} translation failed: {str(e)}", 'error')
        raise


def translate_book_task(task):
    """æ‰§è¡Œç¿»è¯‘ä»»åŠ¡ï¼ˆåå°çº¿ç¨‹)"""
    try:
        task.status = 'translating'
        task.start_time = datetime.now()
        task.emit_log(f"ğŸš€ Starting translation task", 'info')
        task.emit_log(f"ğŸ“š File: {task.filename}", 'info')
        task.emit_log(f"ğŸŒ Target language: {LANGUAGES.get(task.language, task.language)}", 'info')
        
        # åŠ è½½æœ¯è¯­åº“ï¼ˆæ ¹æ®ç”¨æˆ·é€‰æ‹©)
        terminology = None
        curated_count = 0
        if task.use_terminology:
            terminology = load_terminology_db()
            if terminology:
                curated_count = len(terminology)
                task.emit_log(f"ğŸ“š Loaded curated terminology: {curated_count} terms", 'success')
                task.emit_log(f"ğŸ”„ Hybrid mode: Will extract new terms dynamically after first chunk", 'info')
            else:
                task.emit_log(f"âš ï¸  Terminology database not found, will use pure dynamic extraction mode", 'warning')
                terminology = []
        else:
            task.emit_log(f"â„¹ï¸  User chose not to use terminology database", 'info')
        
        # ç¿»è¯‘æ‰€æœ‰å—
        all_translations = []
        prev_context = ""
        
        for chunk in task.chunks_info:
            task.current_chunk = chunk['id']
            
            translation = translate_chunk_web(
                task=task,
                chunk_id=chunk['id'],
                total_chunks=task.total_chunks,
                chunk_content=chunk['content'],
                language=task.language,
                prev_context=prev_context,
                terminology=terminology
            )
            
            all_translations.append({
                'chunk_id': chunk['id'],
                'translation': translation,
                'chapters': chunk['chapters']
            })
            
            # ğŸ”¥ å…³é”®ï¼šä»ç¬¬ä¸€å—æå–æ–°æœ¯è¯­ï¼ˆæ··åˆæ¨¡å¼)
            if chunk['id'] == 1 and terminology is not None:
                task.emit_log(f"ğŸ” Extracting new terms from first chunk...", 'info')
                extracted_terms = extract_terminology_from_chunk(translation, chunk['content'])
                
                # è¿‡æ»¤å·²å­˜åœ¨çš„æœ¯è¯­
                new_terms = [t for t in extracted_terms if t not in terminology]
                
                if new_terms:
                    terminology.extend(new_terms)
                    task.emit_log(f"âœ¨ Extracted {len(new_terms)} new terms (total: {len(terminology)})", 'success')
                    if len(new_terms) <= 10:
                        task.emit_log(f"   New: {', '.join(new_terms[:10])}", 'info')
                    else:
                        task.emit_log(f"   Sample: {', '.join(new_terms[:5])}...", 'info')
                else:
                    task.emit_log(f"âœ… First chunk terms already covered, no supplement needed", 'success')
            
            # æ›´æ–°ä¸Šä¸‹æ–‡
            prev_context = get_context_from_previous(translation)
            
            # å¢é‡ä¿å­˜
            output_file = app.config['OUTPUT_FOLDER'] / f"{task.task_id}_{task.language}.md"
            with open(output_file, 'w', encoding='utf-8') as f:
                for t in all_translations:
                    f.write(f"\n\n<!-- Chunk {t['chunk_id']}: {', '.join(t['chapters'])} -->\n\n")
                    f.write(t['translation'])
            
            task.emit_log(f"ğŸ’¾ Progress saved ({chunk['id']}/{task.total_chunks})", 'info')
        
        # æœ€ç»ˆåˆå¹¶
        final_translation = '\n\n'.join(t['translation'] for t in all_translations)
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        output_file = app.config['OUTPUT_FOLDER'] / f"{task.task_id}_{task.language}_final.md"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(final_translation)
        
        task.result_file = str(output_file)
        task.status = 'completed'
        task.end_time = datetime.now()
        task.progress = 100
        
        elapsed = (task.end_time - task.start_time).total_seconds()
        task.emit_log(f"ğŸ‰ Translation completed!", 'success')
        task.emit_log(f"ğŸ“Š Total: {len(final_translation):,} characters", 'success')
        task.emit_log(f"â±ï¸  Time elapsed: {elapsed:.0f} seconds", 'success')
        task.emit_progress(100, 100)
        
    except Exception as e:
        task.status = 'failed'
        task.error = str(e)
        task.emit_log(f"ğŸ’¥ translation failed: {str(e)}", 'error')


# ============ Flask è·¯ç”± ============

@app.route('/')
def index():
    """ä¸»é¡µ"""
    return render_template('index.html')


@app.route('/api/upload', methods=['POST'])
def upload_file():
    """ä¸Šä¼ æ–‡ä»¶ï¼ˆæ”¯æŒ .md å’Œ .docxï¼‰"""
    if 'file' not in request.files:
        return jsonify({'error': 'æ²¡æœ‰æ–‡ä»¶'}), 400
    
    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'æ–‡ä»¶åä¸ºç©º'}), 400
    
    # æ£€æŸ¥æ–‡ä»¶ç±»å‹
    file_ext = file.filename.rsplit('.', 1)[-1].lower()
    if file_ext not in ['md', 'docx']:
        return jsonify({'error': 'åªæ”¯æŒ Markdown (.md) å’Œ Word (.docx) æ–‡ä»¶'}), 400
    
    # ä¿å­˜æ–‡ä»¶
    filename = secure_filename(file.filename)
    task_id = f"{int(time.time())}_{filename.rsplit('.', 1)[0]}"
    filepath = app.config['UPLOAD_FOLDER'] / f"{task_id}_{filename}"
    file.save(filepath)
    
    # è¯»å–å†…å®¹
    try:
        if file_ext == 'docx':
            # è½¬æ¢ DOCX ä¸º Markdown
            content = convert_docx_to_markdown(filepath)
            # ä¿å­˜è½¬æ¢åçš„ Markdown ç‰ˆæœ¬
            md_filepath = app.config['UPLOAD_FOLDER'] / f"{task_id}_converted.md"
            with open(md_filepath, 'w', encoding='utf-8') as f:
                f.write(content)
            conversion_note = "âœ… Word document converted to Markdown automatically"
        else:
            # ç›´æ¥è¯»å– Markdown
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
            conversion_note = None
    except Exception as e:
        return jsonify({'error': f'File processing failed: {str(e)}'}), 500
    
    response_data = {
        'task_id': task_id,
        'filename': filename,
        'size': len(content),
        'chars': len(content),
        'words': len(content.split()),
        'file_type': file_ext
    }
    
    if conversion_note:
        response_data['conversion_note'] = conversion_note
    
    return jsonify(response_data)


@app.route('/api/analyze/<task_id>', methods=['POST'])
def analyze_file(task_id):
    """åˆ†ææ–‡ä»¶å¹¶è§„åˆ’åˆ†å—"""
    data = request.json
    language = data.get('language', 'Japanese')
    
    # æŸ¥æ‰¾ä¸Šä¼ çš„æ–‡ä»¶ï¼ˆä¼˜å…ˆæŸ¥æ‰¾è½¬æ¢åçš„ .md æ–‡ä»¶ï¼‰
    converted_file = app.config['UPLOAD_FOLDER'] / f"{task_id}_converted.md"
    if converted_file.exists():
        filepath = converted_file
    else:
        files = list(app.config['UPLOAD_FOLDER'].glob(f"{task_id}_*"))
        if not files:
            return jsonify({'error': 'æ–‡ä»¶ä¸å­˜åœ¨'}), 404
        filepath = files[0]
    
    # è¯»å–å†…å®¹
    with open(filepath, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # åˆ†æç« èŠ‚å’Œåˆ†å—
    chapters = extract_chapters(content)
    chunks = plan_chunks(chapters, content)
    
    # åˆ›å»ºä»»åŠ¡
    task = TranslationTask(task_id, filepath.name, language)
    task.source_content = content
    task.total_chunks = len(chunks)
    task.chunks_info = chunks
    task.status = 'analyzed'
    
    tasks[task_id] = task
    
    # è¿”å›åˆ†å—ä¿¡æ¯
    chunks_summary = []
    for chunk in chunks:
        chunks_summary.append({
            'id': chunk['id'],
            'size': chunk['size'],
            'chapters': chunk['chapters'],
            'has_prologue': chunk.get('has_prologue', False),
            'has_epilogue': chunk.get('has_epilogue', False)
        })
    
    return jsonify({
        'task_id': task_id,
        'total_chunks': len(chunks),
        'total_chars': len(content),
        'chunks': chunks_summary
    })


@app.route('/api/translate/<task_id>', methods=['POST'])
def start_translation(task_id):
    """å¼€å§‹ç¿»è¯‘"""
    if task_id not in tasks:
        return jsonify({'error': 'ä»»åŠ¡ä¸å­˜åœ¨'}), 404
    
    task = tasks[task_id]
    
    if task.status != 'analyzed':
        return jsonify({'error': 'ä»»åŠ¡çŠ¶æ€é”™è¯¯'}), 400
    
    # è·å–é…ç½®é€‰é¡¹
    data = request.json or {}
    task.use_terminology = data.get('use_terminology', True)
    
    # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œç¿»è¯‘
    thread = threading.Thread(target=translate_book_task, args=(task,))
    thread.daemon = True
    thread.start()
    
    return jsonify({'status': 'started', 'task_id': task_id})


@app.route('/api/status/<task_id>')
def get_status(task_id):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    if task_id not in tasks:
        return jsonify({'error': 'ä»»åŠ¡ä¸å­˜åœ¨'}), 404
    
    task = tasks[task_id]
    
    return jsonify({
        'task_id': task_id,
        'status': task.status,
        'progress': task.progress,
        'current_chunk': task.current_chunk,
        'total_chunks': task.total_chunks,
        'result_file': task.result_file,
        'error': task.error
    })


@app.route('/api/download/<task_id>')
def download_result(task_id):
    """ä¸‹è½½ç¿»è¯‘ç»“æœ"""
    if task_id not in tasks:
        return jsonify({'error': 'ä»»åŠ¡ä¸å­˜åœ¨'}), 404
    
    task = tasks[task_id]
    
    if task.status != 'completed' or not task.result_file:
        return jsonify({'error': 'ç¿»è¯‘æœªå®Œæˆ'}), 400
    
    return send_file(
        task.result_file,
        as_attachment=True,
        download_name=f"{task.filename.split('.')[0]}_{task.language}.md"
    )


@app.route('/api/preview/<task_id>')
def preview_result(task_id):
    """é¢„è§ˆç¿»è¯‘ç»“æœå†…å®¹"""
    if task_id not in tasks:
        return jsonify({'error': 'ä»»åŠ¡ä¸å­˜åœ¨'}), 404
    
    task = tasks[task_id]
    
    if task.status != 'completed' or not task.result_file:
        return jsonify({'error': 'ç¿»è¯‘æœªå®Œæˆ'}), 400
    
    try:
        with open(task.result_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        return jsonify({
            'success': True,
            'content': content,
            'filename': f"{task.filename.split('.')[0]}_{task.language}.md",
            'language': task.language
        })
    except Exception as e:
        return jsonify({'error': f'è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}'}), 500


@app.route('/api/preview-source/<task_id>')
def preview_source(task_id):
    """é¢„è§ˆä¸Šä¼ çš„æºæ–‡ä»¶å†…å®¹"""
    if task_id not in tasks:
        return jsonify({'error': 'ä»»åŠ¡ä¸å­˜åœ¨'}), 404
    
    task = tasks[task_id]
    
    if not task.source_content:
        return jsonify({'error': 'æºæ–‡ä»¶å†…å®¹ä¸å¯ç”¨'}), 400
    
    try:
        return jsonify({
            'success': True,
            'content': task.source_content,
            'filename': task.filename,
            'language': 'English (source)'
        })
    except Exception as e:
        return jsonify({'error': f'è¯»å–æºæ–‡ä»¶å¤±è´¥: {str(e)}'}), 500


@app.route('/api/preview-chunk/<task_id>/<int:chunk_id>')
def preview_chunk(task_id, chunk_id):
    """é¢„è§ˆç‰¹å®š chunk çš„å†…å®¹"""
    if task_id not in tasks:
        return jsonify({'error': 'ä»»åŠ¡ä¸å­˜åœ¨'}), 404
    
    task = tasks[task_id]
    
    if not task.chunks_info:
        return jsonify({'error': 'Chunk ä¿¡æ¯ä¸å¯ç”¨ï¼Œè¯·å…ˆåˆ†ææ–‡ä»¶'}), 400
    
    # æŸ¥æ‰¾å¯¹åº”çš„ chunk
    chunk_data = None
    for chunk in task.chunks_info:
        if chunk['id'] == chunk_id:
            chunk_data = chunk
            break
    
    if not chunk_data:
        return jsonify({'error': f'Chunk {chunk_id} ä¸å­˜åœ¨'}), 404
    
    try:
        # è·å– chunk çš„å®Œæ•´å†…å®¹
        content = chunk_data.get('content', '')
        
        if not content:
            # å¦‚æœæ²¡æœ‰ä¿å­˜å†…å®¹ï¼Œå°è¯•ä»æºæ–‡ä»¶ä¸­æå–
            if chunk_data.get('start_pos') is not None and chunk_data.get('end_pos') is not None:
                content = task.source_content[chunk_data['start_pos']:chunk_data['end_pos']]
            else:
                content = f"# Chunk {chunk_id}\n\nContent not available"
        
        return jsonify({
            'success': True,
            'content': content,
            'chunk_id': chunk_id,
            'chapters': chunk_data.get('chapters', []),
            'size': chunk_data.get('size', 0),
            'has_prologue': chunk_data.get('has_prologue', False),
            'has_epilogue': chunk_data.get('has_epilogue', False)
        })
    except Exception as e:
        return jsonify({'error': f'è¯»å– Chunk å¤±è´¥: {str(e)}'}), 500


@app.route('/api/terminology')
def get_terminology():
    """è·å–æœ¯è¯­æ•°æ®åº“ï¼ˆåŒ…å«åŠ¨æ€æå–è¯´æ˜)"""
    term_file = Path(__file__).parent / 'terminology_curated.json'
    
    if not term_file.exists():
        return jsonify({
            'error': 'Terminology database not found',
            'path': str(term_file),
            'note': 'Will use dynamic extraction from first chunk if enabled'
        }), 404
    
    try:
        with open(term_file, 'r', encoding='utf-8') as f:
            terminology = json.load(f)
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total': sum(len(terms) for terms in terminology.values()),
            'categories': {}
        }
        
        for category, terms in terminology.items():
            stats['categories'][category] = len(terms)
        
        return jsonify({
            'terminology': terminology,
            'stats': stats,
            'mode': 'hybrid',
            'description': 'Curated terms + dynamic extraction from first chunk'
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/model-info')
def get_model_info_api():
    """è·å–å½“å‰æ¨¡å‹é…ç½®ä¿¡æ¯"""
    try:
        info = get_model_info()
        return jsonify({
            'success': True,
            'model_info': info
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/models')
def get_models_list():
    """è·å–æ‰€æœ‰å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    try:
        models = list_available_models()
        return jsonify({
            'success': True,
            'active_model': ACTIVE_MODEL,
            'models': models
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


# ============ WebSocket äº‹ä»¶ ============

@socketio.on('connect')
def handle_connect():
    """å®¢æˆ·ç«¯è¿æ¥"""
    print(f"âœ… Client connected")
    return True


@socketio.on('disconnect')
def handle_disconnect():
    """å®¢æˆ·ç«¯æ–­å¼€"""
    print(f"âŒ Client disconnected")
    return True


@socketio.on('join')
def handle_join(data):
    """åŠ å…¥ä»»åŠ¡æˆ¿é—´"""
    task_id = data.get('task_id')
    if task_id:
        # ä½¿ç”¨ Flask-SocketIO çš„ join_room
        from flask_socketio import join_room
        join_room(task_id)
        print(f"ğŸ“¥ Client joined room: {task_id}")
        return {'status': 'joined', 'task_id': task_id}
    return {'status': 'error', 'message': 'No task_id provided'}


# ============ å¯åŠ¨æœåŠ¡ ============

if __name__ == '__main__':
    print(f"""
{'='*80}
ğŸš€ Master Translator Web - Hackathon Demo
{'='*80}
ğŸ“¡ æœåŠ¡å™¨å¯åŠ¨åœ¨: http://localhost:5001
ğŸ”Œ WebSocket å·²å¯ç”¨
ğŸ“ ä¸Šä¼ ç›®å½•: {app.config['UPLOAD_FOLDER']}
ğŸ“ è¾“å‡ºç›®å½•: {app.config['OUTPUT_FOLDER']}
{'='*80}
""")
    socketio.run(app, debug=True, host='0.0.0.0', port=5001)
